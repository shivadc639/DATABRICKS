{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ef4045e-d6ae-49d0-a8fa-fc5c487cac6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Stream Customers data from cloud to Delta Lake\n",
    "#### 1. Read files from cloud storage using DataStreamReader API\n",
    "#### 2. Transform the dataframe to add following columns.\n",
    "- ##### Cloud file_path\n",
    "- ##### Ingestion_date \n",
    "#### 3. Write the transformed data stream to Delta Lake table.      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46776fcf-0aaa-467e-be6c-a732718ae4e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1. Read files from cloud storage using DataStreamReader API "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c6e4b14-ef6b-416c-8461-5b1a301c92be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- ##### Streaming doesnt support schema evalution so schema for the tables has to be explicitly defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "779dfc9e-099b-4e2c-82cc-6949b2ee4d96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, DateType, TimestampType, StringType\n",
    "\n",
    "customers_schema = StructType(\n",
    "                              fields = [StructField(\"Customer_Id\", IntegerType()),\n",
    "                                        StructField(\"Customer_Name\", StringType()),\n",
    "                                        StructField(\"date_of_birth\", DateType()),\n",
    "                                        StructField(\"telephone\", StringType()),\n",
    "                                        StructField(\"email\", StringType()),\n",
    "                                        StructField(\"member_since\", DateType()),\n",
    "                                        StructField(\"created_timestamp\", TimestampType())\n",
    "                                        ]\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43139969-383c-4260-b1ba-c3d9bea95d9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_customers_stream = (spark\n",
    "                        .readStream\n",
    "                        .format('json')\n",
    "                        .schema(customers_schema)\n",
    "                        .load('/Volumes/gizmobox/landing/operations_volume/customers_stream/')\n",
    "                      )\n",
    "display(df_customers_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b14a71a8-3951-48f7-93d4-c1ccfc31595c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2. Transform the dataframe to add following columns.\n",
    "- #### Cloud file_path\n",
    "- #### Ingestion_date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6eed84df-7d90-4c54-ab1c-d445242368ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, current_timestamp\n",
    "\n",
    "df_customers_stream_transformed = (df_customers_stream\n",
    "                                                    .withColumn('file_path', col('_metadata.file_path'))\n",
    "                                                    .withColumn('ingestion_date', current_timestamp())\n",
    "                                  )\n",
    "display(df_customers_stream_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d3b1102-bf61-49f1-b143-1df3c673543f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 3. Write the transformed data stream to Delta Lake table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b572549d-7037-4bf6-928e-c3772db4c8cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customers_streaming_query = (df_customers_stream_transformed\n",
    "                                      .writeStream\n",
    "                                      .format('delta')\n",
    "                                      .option('checkpointLocation', '/Volumes/gizmobox/landing/operations_volume/customers_stream/_customers_checkpoint_location')\n",
    "                                      .toTable('gizmobox.bronze.customers_stream')\n",
    "                            )\n",
    "display(customers_streaming_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0ae9536-952c-4fc4-8b6e-857e58c249fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- #### writeStream queries will not stop automatically. It has to be manually terminated. It can be manually terminated by clicking on `Terminate` icon in the query block or by using `.stop()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "514ef903-335d-409c-a4ba-d42fe9ee8742",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customers_streaming_query.stop()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1. Ingest Customers Stream",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
